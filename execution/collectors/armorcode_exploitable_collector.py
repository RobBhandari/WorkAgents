#!/usr/bin/env python3
"""
ArmorCode Exploitable Vulnerability Collector

Fetches High + Critical findings across all baseline products and identifies
'exploitable' ones using the armorcode.aati.inthewild tag (value > 0).

Queries are scoped per source bucket (INFRASTRUCTURE / CODE / CLOUD) so
each query covers a small slice of findings — avoiding the 10k API limit
while giving exact counts for every bucket.

Saves results to .tmp/observatory/exploitable_history.json as weekly snapshots.

Usage:
    python execution/collectors/armorcode_exploitable_collector.py
"""

import json
import logging
import time
from datetime import datetime
from pathlib import Path

from dotenv import load_dotenv

from execution.core import get_logger
from execution.domain.security import BUCKET_ORDER, SOURCE_BUCKET_MAP
from execution.http_client import post
from execution.secure_config import get_config

load_dotenv()

logger = get_logger(__name__)

HISTORY_PATH = Path(".tmp/observatory/exploitable_history.json")
ARMORCODE_PAGE_LIMIT = 100  # ArmorCode hard limit: 100 pages × 100 items = 10k max

# Build bucket → [source strings] mapping from SOURCE_BUCKET_MAP
SOURCES_BY_BUCKET: dict[str, list[str]] = {}
for _src, _bucket in SOURCE_BUCKET_MAP.items():
    SOURCES_BY_BUCKET.setdefault(_bucket, []).append(_src)


def _get_headers() -> dict:
    """Build ArmorCode API auth headers."""
    api_key = get_config().get_armorcode_config().api_key
    return {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}


def _get_graphql_url() -> str:
    base_url = get_config().get_armorcode_config().base_url
    return f"{base_url.rstrip('/')}/api/graphql"


def _load_baseline_products() -> list[str]:
    """Load product names from ArmorCode baseline file."""
    candidates = [
        Path("data/armorcode_baseline.json"),
        Path(".tmp/armorcode_baseline.json"),
    ]
    for path in candidates:
        if path.exists():
            data = json.loads(path.read_text(encoding="utf-8"))
            products = data.get("products", [])
            logger.info(f"Loaded {len(products)} products from {path}")
            return products
    raise FileNotFoundError("ArmorCode baseline not found in data/ or .tmp/")


def _get_product_ids(product_names: list[str]) -> dict[str, str]:
    """
    Resolve product names to ArmorCode product IDs via GraphQL.

    Returns:
        Dict mapping product_name -> product_id
    """
    graphql_url = _get_graphql_url()
    headers = _get_headers()
    name_to_id: dict[str, str] = {}
    name_set = {n.lower() for n in product_names}

    for page in range(1, 20):
        query = f"""
        {{
          products(page: {page}, size: 100) {{
            products {{ id name }}
            pageInfo {{ hasNext }}
          }}
        }}
        """
        try:
            resp = post(graphql_url, headers=headers, json={"query": query}, timeout=30)
            if resp.status_code != 200:
                logger.error(f"Products query HTTP {resp.status_code}")
                break
            data = resp.json()
            result = data.get("data", {}).get("products", {})
            for p in result.get("products", []):
                if p["name"].lower() in name_set:
                    name_to_id[p["name"]] = p["id"]
            if not result.get("pageInfo", {}).get("hasNext", False):
                break
        except Exception as e:
            logger.error(f"Error fetching products page {page}: {e}")
            break

    logger.info(f"Resolved {len(name_to_id)}/{len(product_names)} product IDs")
    return name_to_id


def _is_exploitable(tags: list) -> bool:
    """Return True if armorcode.aati.inthewild > 0."""
    for tag in tags or []:
        if tag.startswith("armorcode.aati.inthewild:"):
            val = tag.split(":", 1)[1]
            if val.isdigit() and int(val) > 0:
                return True
    return False


def _fetch_exploitable_for_bucket(
    product_id: str,
    product_name: str,
    severity: str,
    sources: list[str],
    bucket: str,
) -> tuple[list[dict], bool]:
    """
    Fetch exploitable findings for one product + severity + source bucket.

    Scoping by source keeps each query well under the 10k result limit so
    we get exact counts per bucket without partial-data flags.

    Returns:
        (exploitable_findings, data_complete)
        data_complete=False if the 10k limit was still reached for this bucket
    """
    graphql_url = _get_graphql_url()
    headers = _get_headers()
    exploitable: list[dict] = []
    page = 1
    data_complete = True
    last_has_next = False

    sources_str = ", ".join(f'"{s}"' for s in sources)

    while page <= ARMORCODE_PAGE_LIMIT:
        query = f"""
        {{
          findings(
            page: {page}
            size: 100
            findingFilter: {{
              product: [{product_id}]
              severity: [{severity}]
              source: [{sources_str}]
              status: ["OPEN", "CONFIRMED"]
            }}
          ) {{
            findings {{
              id severity status source cve tags
            }}
            pageInfo {{ hasNext totalElements }}
          }}
        }}
        """
        try:
            resp = post(graphql_url, headers=headers, json={"query": query}, timeout=60)
        except Exception as e:
            logger.error(f"  [{product_name}] {bucket}/{severity} page {page} request failed: {e}")
            break

        if resp.status_code == 429:
            retry_after = int(resp.headers.get("Retry-After", 60))
            logger.warning(f"  Rate limited — waiting {retry_after}s")
            time.sleep(retry_after)
            continue

        if resp.status_code != 200:
            logger.error(f"  [{product_name}] {bucket}/{severity} HTTP {resp.status_code}")
            break

        data = resp.json()
        if "errors" in data:
            err_msg = str(data["errors"])
            if "beyond 10k" in err_msg or "10000" in err_msg:
                logger.warning(f"  [{product_name}] {bucket}/{severity}: hit 10k result limit at page {page}")
                data_complete = False
            else:
                logger.error(f"  [{product_name}] {bucket}/{severity} GraphQL error: {data['errors']}")
            break

        findings_data = data.get("data", {}).get("findings", {})
        page_findings = findings_data.get("findings", [])
        page_info = findings_data.get("pageInfo", {})

        for f in page_findings:
            if _is_exploitable(f.get("tags") or []):
                exploitable.append(f)

        if page == 1:
            total = page_info.get("totalElements", 0)
            if total > 0:
                logger.info(f"  [{product_name}] {bucket}/{severity}: {total} total findings")

        last_has_next = page_info.get("hasNext", False)
        if not last_has_next:
            break

        page += 1

    # Exited due to page cap with more pages remaining
    if page > ARMORCODE_PAGE_LIMIT and last_has_next:
        logger.warning(
            f"  [{product_name}] {bucket}/{severity}: hit {ARMORCODE_PAGE_LIMIT}-page limit "
            f"({ARMORCODE_PAGE_LIMIT * 100:,} results scanned) — data may be partial"
        )
        data_complete = False

    return exploitable, data_complete


def _collect_product_exploitables(product_name: str, product_id: str) -> dict:
    """
    Collect exploitable findings for one product, broken down by source bucket.

    Queries each bucket (INFRASTRUCTURE / CODE / CLOUD) separately so each
    query targets a small subset of findings — avoiding the 10k API limit and
    giving exact counts per bucket.

    Returns:
        Dict with critical, high, high_data_complete, buckets, top_cves
    """
    logger.info(f"Collecting exploitable findings for: {product_name}")

    buckets: dict[str, dict[str, int]] = {b: {"critical": 0, "high": 0} for b in BUCKET_ORDER}
    all_cves: set[str] = set()
    total_critical = 0
    total_high = 0
    high_complete = True

    for bucket, sources in SOURCES_BY_BUCKET.items():
        if bucket == "Other":
            continue  # Skip unknown sources

        # Critical findings for this bucket (should always be small, < 10k)
        crit_findings, _ = _fetch_exploitable_for_bucket(product_id, product_name, "Critical", sources, bucket)
        # High findings for this bucket (per-bucket scope keeps this manageable)
        high_findings, h_complete = _fetch_exploitable_for_bucket(product_id, product_name, "High", sources, bucket)

        if not h_complete:
            high_complete = False

        crit_count = len(crit_findings)
        high_count = len(high_findings)

        if crit_count + high_count > 0:
            buckets[bucket]["critical"] = crit_count
            buckets[bucket]["high"] = high_count

        total_critical += crit_count
        total_high += high_count

        # Collect unique CVEs from all exploitable findings in this bucket
        for f in crit_findings + high_findings:
            for cve in f.get("cve") or []:
                if cve:
                    all_cves.add(cve)

    logger.info(
        f"  {product_name}: {total_critical} exploitable critical, "
        f"{total_high} exploitable high" + ("" if high_complete else " [HIGH data partial - 10k limit]")
    )

    return {
        "critical": total_critical,
        "high": total_high,
        "high_data_complete": high_complete,
        "total": total_critical + total_high,
        "buckets": {k: v for k, v in buckets.items() if v["critical"] + v["high"] > 0},
        "top_cves": sorted(all_cves),
    }


def collect_exploitable_metrics() -> dict:
    """
    Collect exploitable findings across all baseline products.

    Returns:
        Metrics dict ready to append to exploitable_history.json
    """
    product_names = _load_baseline_products()
    name_to_id = _get_product_ids(product_names)

    product_breakdown: dict[str, dict] = {}
    total_critical = 0
    total_high = 0
    any_incomplete = False

    for name in product_names:
        product_id = name_to_id.get(name)
        if not product_id:
            logger.warning(f"No product ID for '{name}' — skipping")
            product_breakdown[name] = {
                "critical": 0,
                "high": 0,
                "high_data_complete": True,
                "total": 0,
                "buckets": {},
                "top_cves": [],
            }
            continue

        result = _collect_product_exploitables(name, product_id)
        product_breakdown[name] = result
        total_critical += result["critical"]
        total_high += result["high"]
        if not result["high_data_complete"]:
            any_incomplete = True

    now = datetime.utcnow()
    return {
        "week_date": now.strftime("%Y-%m-%d"),
        "week_number": now.isocalendar()[1],
        "metrics": {
            "current_total": total_critical + total_high,
            "severity_breakdown": {
                "critical": total_critical,
                "high": total_high,
                "total": total_critical + total_high,
            },
            "high_data_complete": not any_incomplete,
            "product_breakdown": product_breakdown,
            "collected_at": now.isoformat(),
        },
    }


def _save_to_history(week_data: dict) -> None:
    """Append week_data to exploitable_history.json."""
    HISTORY_PATH.parent.mkdir(parents=True, exist_ok=True)

    if HISTORY_PATH.exists():
        history = json.loads(HISTORY_PATH.read_text(encoding="utf-8"))
    else:
        history = {"weeks": []}

    history["weeks"].append(week_data)
    HISTORY_PATH.write_text(json.dumps(history, indent=2), encoding="utf-8")
    logger.info(f"Saved exploitable history to {HISTORY_PATH}")


def main() -> None:
    """Entry point for CI and local execution."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
    )
    logger.info("=" * 60)
    logger.info("Exploitable Vulnerability Collector")
    logger.info("=" * 60)

    week_data = collect_exploitable_metrics()
    _save_to_history(week_data)

    total = week_data["metrics"]["severity_breakdown"]["total"]
    crit = week_data["metrics"]["severity_breakdown"]["critical"]
    high = week_data["metrics"]["severity_breakdown"]["high"]
    logger.info(f"Done — {total} exploitable findings ({crit} critical, {high} high)")


if __name__ == "__main__":
    main()
