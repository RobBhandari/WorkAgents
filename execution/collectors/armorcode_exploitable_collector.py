#!/usr/bin/env python3
"""
ArmorCode Exploitable Vulnerability Collector

Fetches High + Critical findings across all baseline products and identifies
'exploitable' ones using the armorcode.aati.inthewild tag (value > 0).

Saves results to .tmp/observatory/exploitable_history.json as weekly snapshots.

Note: ArmorCode GraphQL has a hard 10k result limit. For High findings this may
be hit on large products. When reached, high_data_complete = false is recorded.

Usage:
    python execution/collectors/armorcode_exploitable_collector.py
"""

import json
import logging
import time
from collections import defaultdict
from datetime import datetime
from pathlib import Path

from dotenv import load_dotenv

from execution.core import get_logger
from execution.domain.security import BUCKET_ORDER, SOURCE_BUCKET_MAP
from execution.http_client import post
from execution.secure_config import get_config

load_dotenv()

logger = get_logger(__name__)

HISTORY_PATH = Path(".tmp/observatory/exploitable_history.json")
ARMORCODE_PAGE_LIMIT = 100  # ArmorCode hard limit: 100 pages × 100 items = 10k max


def _get_headers() -> dict:
    """Build ArmorCode API auth headers."""
    api_key = get_config().get_armorcode_config().api_key
    return {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}


def _get_graphql_url() -> str:
    base_url = get_config().get_armorcode_config().base_url
    return f"{base_url.rstrip('/')}/api/graphql"


def _load_baseline_products() -> list[str]:
    """Load product names from ArmorCode baseline file."""
    candidates = [
        Path("data/armorcode_baseline.json"),
        Path(".tmp/armorcode_baseline.json"),
    ]
    for path in candidates:
        if path.exists():
            data = json.loads(path.read_text(encoding="utf-8"))
            products = data.get("products", [])
            logger.info(f"Loaded {len(products)} products from {path}")
            return products
    raise FileNotFoundError("ArmorCode baseline not found in data/ or .tmp/")


def _get_product_ids(product_names: list[str]) -> dict[str, str]:
    """
    Resolve product names to ArmorCode product IDs via GraphQL.

    Returns:
        Dict mapping product_name -> product_id
    """
    graphql_url = _get_graphql_url()
    headers = _get_headers()
    name_to_id: dict[str, str] = {}
    name_set = {n.lower() for n in product_names}

    for page in range(1, 20):
        query = f"""
        {{
          products(page: {page}, size: 100) {{
            products {{ id name }}
            pageInfo {{ hasNext }}
          }}
        }}
        """
        try:
            resp = post(graphql_url, headers=headers, json={"query": query}, timeout=30)
            if resp.status_code != 200:
                logger.error(f"Products query HTTP {resp.status_code}")
                break
            data = resp.json()
            result = data.get("data", {}).get("products", {})
            for p in result.get("products", []):
                if p["name"].lower() in name_set:
                    name_to_id[p["name"]] = p["id"]
            if not result.get("pageInfo", {}).get("hasNext", False):
                break
        except Exception as e:
            logger.error(f"Error fetching products page {page}: {e}")
            break

    logger.info(f"Resolved {len(name_to_id)}/{len(product_names)} product IDs")
    return name_to_id


def _is_exploitable(tags: list) -> bool:
    """Return True if armorcode.aati.inthewild > 0."""
    for tag in tags or []:
        if tag.startswith("armorcode.aati.inthewild:"):
            val = tag.split(":", 1)[1]
            if val.isdigit() and int(val) > 0:
                return True
    return False


def _fetch_findings_for_product(product_id: str, product_name: str, severity: str) -> tuple[list[dict], bool]:
    """
    Fetch all findings for one product + severity, returning exploitable ones.

    Returns:
        (exploitable_findings, data_complete)
        data_complete=False when the 10k result limit was hit
    """
    graphql_url = _get_graphql_url()
    headers = _get_headers()
    exploitable: list[dict] = []
    page = 1
    data_complete = True
    last_has_next = False

    while page <= ARMORCODE_PAGE_LIMIT:
        query = f"""
        {{
          findings(
            page: {page}
            size: 100
            findingFilter: {{
              product: [{product_id}]
              severity: [{severity}]
              status: ["OPEN", "CONFIRMED"]
            }}
          ) {{
            findings {{
              id severity status source cve tags
            }}
            pageInfo {{ hasNext totalElements }}
          }}
        }}
        """
        try:
            resp = post(graphql_url, headers=headers, json={"query": query}, timeout=60)
        except Exception as e:
            logger.error(f"  [{product_name}] {severity} page {page} request failed: {e}")
            break

        if resp.status_code == 429:
            retry_after = int(resp.headers.get("Retry-After", 60))
            logger.warning(f"  Rate limited — waiting {retry_after}s")
            time.sleep(retry_after)
            continue

        if resp.status_code != 200:
            logger.error(f"  [{product_name}] HTTP {resp.status_code}")
            break

        data = resp.json()
        if "errors" in data:
            # Check for 10k limit error
            err_msg = str(data["errors"])
            if "beyond 10k" in err_msg or "10000" in err_msg:
                logger.warning(f"  [{product_name}] {severity}: hit 10k result limit at page {page}")
                data_complete = False
            else:
                logger.error(f"  [{product_name}] GraphQL error: {data['errors']}")
            break

        findings_data = data.get("data", {}).get("findings", {})
        page_findings = findings_data.get("findings", [])
        page_info = findings_data.get("pageInfo", {})

        for f in page_findings:
            if _is_exploitable(f.get("tags") or []):
                exploitable.append(f)

        if page == 1:
            total = page_info.get("totalElements", 0)
            logger.info(f"  [{product_name}] {severity}: {total} total findings")

        last_has_next = page_info.get("hasNext", False)
        if not last_has_next:
            break

        page += 1

    # If we exited because we hit the page limit (not because hasNext=False),
    # there are still unfetched pages — mark data as incomplete
    if page > ARMORCODE_PAGE_LIMIT and last_has_next:
        logger.warning(
            f"  [{product_name}] {severity}: hit {ARMORCODE_PAGE_LIMIT}-page limit "
            f"({ARMORCODE_PAGE_LIMIT * 100:,} results scanned) — data may be partial"
        )
        data_complete = False

    return exploitable, data_complete


def _collect_product_exploitables(product_name: str, product_id: str) -> dict:
    """
    Collect exploitable findings for one product across Critical + High.

    Returns:
        Dict with critical, high, high_data_complete, buckets, top_cves
    """
    logger.info(f"Collecting exploitable findings for: {product_name}")

    # Critical (exact — well under 10k limit)
    crit_findings, _ = _fetch_findings_for_product(product_id, product_name, "Critical")

    # High (may hit 10k limit)
    high_findings, high_complete = _fetch_findings_for_product(product_id, product_name, "High")

    all_exploitable = crit_findings + high_findings

    # Count by severity
    critical_count = sum(1 for f in all_exploitable if f.get("severity", "").upper() == "CRITICAL")
    high_count = sum(1 for f in all_exploitable if f.get("severity", "").upper() == "HIGH")

    # Bucket breakdown
    buckets: dict[str, dict[str, int]] = {b: {"critical": 0, "high": 0} for b in BUCKET_ORDER}
    for f in all_exploitable:
        source = f.get("source", "") or ""
        bucket = SOURCE_BUCKET_MAP.get(source, "Other")
        sev = f.get("severity", "").upper()
        if sev == "CRITICAL":
            buckets[bucket]["critical"] += 1
        elif sev == "HIGH":
            buckets[bucket]["high"] += 1

    # Unique CVEs (flatten list-of-lists)
    cves: set[str] = set()
    for f in all_exploitable:
        for cve in f.get("cve") or []:
            if cve:
                cves.add(cve)

    logger.info(
        f"  {product_name}: {critical_count} exploitable critical, "
        f"{high_count} exploitable high" + ("" if high_complete else " [HIGH data partial - 10k limit]")
    )

    return {
        "critical": critical_count,
        "high": high_count,
        "high_data_complete": high_complete,
        "total": critical_count + high_count,
        "buckets": {k: v for k, v in buckets.items() if v["critical"] + v["high"] > 0},
        "top_cves": sorted(cves),
    }


def collect_exploitable_metrics() -> dict:
    """
    Collect exploitable findings across all baseline products.

    Returns:
        Metrics dict ready to append to exploitable_history.json
    """
    product_names = _load_baseline_products()
    name_to_id = _get_product_ids(product_names)

    product_breakdown: dict[str, dict] = {}
    total_critical = 0
    total_high = 0
    any_incomplete = False

    for name in product_names:
        product_id = name_to_id.get(name)
        if not product_id:
            logger.warning(f"No product ID for '{name}' — skipping")
            product_breakdown[name] = {
                "critical": 0,
                "high": 0,
                "high_data_complete": True,
                "total": 0,
                "buckets": {},
                "top_cves": [],
            }
            continue

        result = _collect_product_exploitables(name, product_id)
        product_breakdown[name] = result
        total_critical += result["critical"]
        total_high += result["high"]
        if not result["high_data_complete"]:
            any_incomplete = True

    now = datetime.utcnow()
    return {
        "week_date": now.strftime("%Y-%m-%d"),
        "week_number": now.isocalendar()[1],
        "metrics": {
            "current_total": total_critical + total_high,
            "severity_breakdown": {
                "critical": total_critical,
                "high": total_high,
                "total": total_critical + total_high,
            },
            "high_data_complete": not any_incomplete,
            "product_breakdown": product_breakdown,
            "collected_at": now.isoformat(),
        },
    }


def _save_to_history(week_data: dict) -> None:
    """Append week_data to exploitable_history.json."""
    HISTORY_PATH.parent.mkdir(parents=True, exist_ok=True)

    if HISTORY_PATH.exists():
        history = json.loads(HISTORY_PATH.read_text(encoding="utf-8"))
    else:
        history = {"weeks": []}

    history["weeks"].append(week_data)
    HISTORY_PATH.write_text(json.dumps(history, indent=2), encoding="utf-8")
    logger.info(f"Saved exploitable history to {HISTORY_PATH}")


def main() -> None:
    """Entry point for CI and local execution."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
    )
    logger.info("=" * 60)
    logger.info("Exploitable Vulnerability Collector")
    logger.info("=" * 60)

    week_data = collect_exploitable_metrics()
    _save_to_history(week_data)

    total = week_data["metrics"]["severity_breakdown"]["total"]
    crit = week_data["metrics"]["severity_breakdown"]["critical"]
    high = week_data["metrics"]["severity_breakdown"]["high"]
    logger.info(f"Done — {total} exploitable findings ({crit} critical, {high} high)")


if __name__ == "__main__":
    main()
