#!/usr/bin/env python3
"""
ArmorCode Exploitable Vulnerability Collector

Fetches CISA KEV (isCISAKEV=true) findings across all products in the configured
ArmorCode hierarchy (ARMORCODE_HIERARCHY env var — required GitHub secret) using
the AQL-based count endpoint. Returns per-product counts for Critical, High, and
Medium severity.

Products are resolved dynamically via productFilter.groups — no static baseline
file needed. New products added to the hierarchy are automatically included.

Uses POST /user/findings/groups/count with aggField=product — returns exact counts
per product in a single API call per severity. No pagination required.

Saves results to .tmp/observatory/exploitable_history.json as weekly snapshots.

Usage:
    python execution/collectors/armorcode_exploitable_collector.py
"""

import json
import logging
import time
from datetime import datetime
from pathlib import Path

from dotenv import load_dotenv

from execution.core import get_logger
from execution.http_client import post
from execution.secure_config import get_config

load_dotenv()

logger = get_logger(__name__)

HISTORY_PATH = Path(".tmp/observatory/exploitable_history.json")

SEVERITIES = ["Critical", "High", "Medium"]


def _get_headers() -> dict:
    """Build ArmorCode API auth headers."""
    api_key = get_config().get_armorcode_config().api_key
    return {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}


def _get_graphql_url() -> str:
    base_url = get_config().get_armorcode_config().base_url
    return f"{base_url.rstrip('/')}/api/graphql"


def _get_count_url() -> str:
    base_url = get_config().get_armorcode_config().base_url
    return f"{base_url.rstrip('/')}/user/findings/groups/count"


def _get_products_by_hierarchy(hierarchy: str) -> dict[str, str]:
    """
    Fetch all products in an ArmorCode hierarchy via productFilter.groups.

    Returns:
        Dict mapping product_name -> product_id
    """
    graphql_url = _get_graphql_url()
    headers = _get_headers()
    name_to_id: dict[str, str] = {}

    for page in range(1, 20):
        query = f"""
        {{
          products(
            page: {page}
            size: 100
            productFilter: {{ groups: ["{hierarchy}"] }}
          ) {{
            products {{ id name }}
            pageInfo {{ hasNext }}
          }}
        }}
        """
        try:
            resp = post(graphql_url, headers=headers, json={"query": query}, timeout=30)
            if resp.status_code != 200:
                logger.error(f"Products query HTTP {resp.status_code}")
                break
            data = resp.json()
            if "errors" in data:
                logger.error(f"Products query GraphQL error: {data['errors']}")
                break
            result = data.get("data", {}).get("products", {})
            for p in result.get("products", []):
                name_to_id[p["name"]] = str(p["id"])
            if not result.get("pageInfo", {}).get("hasNext", False):
                break
        except Exception as e:
            logger.error(f"Error fetching products page {page}: {e}")
            break

    logger.info(f"Found {len(name_to_id)} products in hierarchy '{hierarchy}'")
    return name_to_id


def _count_by_severity(severity: str, hierarchy: str) -> dict[str, int]:
    """
    Fetch exploitable (isCISAKEV=true) counts grouped by product for one severity.

    Uses POST /user/findings/groups/count with aggField=product.
    Returns a single result per product — no pagination required.

    Args:
        severity: "Critical", "High", or "Medium"
        hierarchy: ArmorCode hierarchy value (e.g. "access group/legal")

    Returns:
        Dict mapping product_id_str -> count
    """
    url = _get_count_url()
    headers = _get_headers()
    aql = (
        f"severity = {severity} AND isCISAKEV = true"
        f" AND status IN (Open,Confirm)"
        f" AND hierarchy = 'armorcode.group:{hierarchy}'"
    )
    payload = {
        "aggField": "product",
        "filters": {"aqlQuery": [aql]},
        "ignoreDuplicate": True,
        "ignoreMitigated": None,
    }

    while True:
        try:
            resp = post(url, headers=headers, json=payload, timeout=30)
        except Exception as e:
            logger.error(f"  [{severity}] Request failed: {e}")
            return {}

        if resp.status_code == 429:
            retry_after = int(resp.headers.get("Retry-After", 60))
            logger.warning(f"  Rate limited — waiting {retry_after}s")
            time.sleep(retry_after)
            continue

        if resp.status_code != 200:
            logger.error(f"  [{severity}] HTTP {resp.status_code}: {resp.text[:200]}")
            return {}

        data = resp.json()
        counts = {k: v.get("count", 0) for k, v in data.items() if isinstance(v, dict)}
        total = sum(counts.values())
        logger.info(f"  {severity}: {total} exploitable (across {len(counts)} products)")
        return counts


def collect_exploitable_metrics() -> dict:
    """
    Collect exploitable findings across all products in the hierarchy.

    Makes 4 API calls total:
      1. Products lookup (GraphQL — existing function)
      2. Critical counts (AQL count endpoint)
      3. High counts (AQL count endpoint)
      4. Medium counts (AQL count endpoint)

    Returns:
        Metrics dict ready to append to exploitable_history.json
    """
    hierarchy = get_config().get_optional_env("ARMORCODE_HIERARCHY")
    if not hierarchy:
        raise RuntimeError(
            "ARMORCODE_HIERARCHY env var not set. " "Add it as a GitHub secret and to your local .env file."
        )
    logger.info("Hierarchy: configured (value kept out of logs for security)")

    name_to_id = _get_products_by_hierarchy(hierarchy)
    id_to_name = {v: k for k, v in name_to_id.items()}

    critical_counts = _count_by_severity("Critical", hierarchy)
    high_counts = _count_by_severity("High", hierarchy)
    medium_counts = _count_by_severity("Medium", hierarchy)

    product_breakdown: dict[str, dict] = {}
    total_critical = 0
    total_high = 0
    total_medium = 0

    for product_id, name in id_to_name.items():
        c = critical_counts.get(product_id, 0)
        h = high_counts.get(product_id, 0)
        m = medium_counts.get(product_id, 0)
        total_critical += c
        total_high += h
        total_medium += m
        if c + h + m > 0:
            product_breakdown[name] = {
                "critical": c,
                "high": h,
                "medium": m,
                "total": c + h + m,
            }
            logger.info(f"  {name}: {c} critical, {h} high, {m} medium exploitable")

    now = datetime.utcnow()
    return {
        "week_date": now.strftime("%Y-%m-%d"),
        "week_number": now.isocalendar()[1],
        "metrics": {
            "current_total": total_critical + total_high + total_medium,
            "severity_breakdown": {
                "critical": total_critical,
                "high": total_high,
                "medium": total_medium,
                "total": total_critical + total_high + total_medium,
            },
            "high_data_complete": True,
            "product_breakdown": product_breakdown,
            "collected_at": now.isoformat(),
        },
    }


def _save_to_history(week_data: dict) -> None:
    """Append week_data to exploitable_history.json."""
    HISTORY_PATH.parent.mkdir(parents=True, exist_ok=True)

    if HISTORY_PATH.exists():
        history = json.loads(HISTORY_PATH.read_text(encoding="utf-8"))
    else:
        history = {"weeks": []}

    history["weeks"].append(week_data)
    HISTORY_PATH.write_text(json.dumps(history, indent=2), encoding="utf-8")
    logger.info(f"Saved exploitable history to {HISTORY_PATH}")


def main() -> None:
    """Entry point for CI and local execution."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
    )
    logger.info("=" * 60)
    logger.info("Exploitable Vulnerability Collector (AQL/CISA KEV)")
    logger.info("=" * 60)

    week_data = collect_exploitable_metrics()
    _save_to_history(week_data)

    sev = week_data["metrics"]["severity_breakdown"]
    logger.info(
        f"Done — {sev['total']} exploitable findings "
        f"({sev['critical']} critical, {sev['high']} high, {sev['medium']} medium)"
    )


if __name__ == "__main__":
    main()
