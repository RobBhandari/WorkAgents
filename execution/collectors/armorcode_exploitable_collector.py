#!/usr/bin/env python3
"""
ArmorCode Exploitable Vulnerability Collector

Fetches CISA KEV (isCISAKEV=true) findings across all products in the configured
ArmorCode hierarchy (ARMORCODE_HIERARCHY env var — required GitHub secret) using
the AQL-based count endpoint. Returns per-product counts for Critical, High, and
Medium severity.

Products are resolved from data/armorcode_id_map.json (written from ARMORCODE_ID_MAP
GitHub secret). No GraphQL calls required.

Uses POST /user/findings/groups/count with aggField=product — returns exact counts
per product in a single API call per severity. No pagination required.

Saves results to .tmp/observatory/exploitable_history.json as weekly snapshots.

Usage:
    python execution/collectors/armorcode_exploitable_collector.py
"""

import json
import logging
import time
from datetime import datetime
from pathlib import Path

from dotenv import load_dotenv

from execution.core import get_logger
from execution.domain.security import SOURCE_BUCKET_MAP
from execution.http_client import post
from execution.secure_config import get_config

load_dotenv()

logger = get_logger(__name__)

HISTORY_PATH = Path(".tmp/observatory/exploitable_history.json")
ID_MAP_PATH = Path("data/armorcode_id_map.json")

SEVERITIES = ["Critical", "High", "Medium"]


def _get_headers() -> dict:
    """Build ArmorCode API auth headers."""
    api_key = get_config().get_armorcode_config().api_key
    return {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}


def _get_count_url() -> str:
    base_url = get_config().get_armorcode_config().base_url
    return f"{base_url.rstrip('/')}/user/findings/groups/count"


def _load_id_map() -> dict[str, str]:
    """
    Load product name → ID mapping from data/armorcode_id_map.json.

    Returns:
        Dict mapping product_name -> product_id
    """
    if not ID_MAP_PATH.exists():
        raise FileNotFoundError(
            f"{ID_MAP_PATH} not found. "
            "In CI/CD this is written from the ARMORCODE_ID_MAP secret. "
            "Locally, run: python scripts/fetch_armorcode_id_map.py"
        )
    result: dict[str, str] = json.loads(ID_MAP_PATH.read_text(encoding="utf-8"))
    return result


def _count_by_severity(severity: str, hierarchy: str) -> dict[str, int]:
    """
    Fetch exploitable (isCISAKEV=true) counts grouped by product for one severity.

    Uses POST /user/findings/groups/count with aggField=product.
    Returns a single result per product — no pagination required.

    Args:
        severity: "Critical", "High", or "Medium"
        hierarchy: ArmorCode hierarchy value (e.g. "access group/legal")

    Returns:
        Dict mapping product_id_str -> count
    """
    url = _get_count_url()
    headers = _get_headers()
    sources = ",".join(f"'{s}'" if " " in s or "-" in s else s for s in SOURCE_BUCKET_MAP.keys())
    aql = (
        f"source IN ({sources})"
        f" AND severity = {severity} AND isCISAKEV = true"
        f" AND assetCloudProviders IN (aws,azure)"
        f" AND status IN (Open,Confirm)"
        f" AND hierarchy = 'armorcode.group:{hierarchy}'"
    )
    payload = {
        "aggField": "product",
        "filters": {"aqlQuery": [aql]},
        "ignoreDuplicate": True,
        "ignoreMitigated": None,
    }

    while True:
        try:
            resp = post(url, headers=headers, json=payload, timeout=30)
        except Exception as e:
            logger.error(f"  [{severity}] Request failed: {e}")
            return {}

        if resp.status_code == 429:
            retry_after = int(resp.headers.get("Retry-After", 60))
            logger.warning(f"  Rate limited — waiting {retry_after}s")
            time.sleep(retry_after)
            continue

        if resp.status_code != 200:
            logger.error(f"  [{severity}] HTTP {resp.status_code}: {resp.text[:200]}")
            return {}

        data = resp.json()
        counts = {k: v.get("count", 0) for k, v in data.items() if isinstance(v, dict)}
        total = sum(counts.values())
        logger.info(f"  {severity}: {total} exploitable (across {len(counts)} products)")
        return counts


def collect_exploitable_metrics() -> dict:
    """
    Collect exploitable findings across all products in the hierarchy.

    Makes 3 API calls total (all AQL — no GraphQL):
      1. Critical counts (AQL count endpoint)
      2. High counts (AQL count endpoint)
      3. Medium counts (AQL count endpoint)

    Products are loaded from data/armorcode_id_map.json (from ARMORCODE_ID_MAP secret).
    Zero-pads products absent from AQL results.

    Returns:
        Metrics dict ready to append to exploitable_history.json
    """
    hierarchy = get_config().get_optional_env("ARMORCODE_HIERARCHY")
    if not hierarchy:
        raise RuntimeError(
            "ARMORCODE_HIERARCHY env var not set. " "Add it as a GitHub secret and to your local .env file."
        )
    logger.info("Hierarchy: configured (value kept out of logs for security)")

    name_to_id = _load_id_map()
    id_to_name = {v: k for k, v in name_to_id.items()}
    logger.info(f"Loaded {len(name_to_id)} products from {ID_MAP_PATH}")

    critical_counts = _count_by_severity("Critical", hierarchy)
    high_counts = _count_by_severity("High", hierarchy)
    medium_counts = _count_by_severity("Medium", hierarchy)

    product_breakdown: dict[str, dict] = {}
    total_critical = 0
    total_high = 0
    total_medium = 0

    for product_id, name in id_to_name.items():
        c = critical_counts.get(product_id, 0)
        h = high_counts.get(product_id, 0)
        m = medium_counts.get(product_id, 0)
        total_critical += c
        total_high += h
        total_medium += m
        product_breakdown[product_id] = {
            "critical": c,
            "high": h,
            "medium": m,
            "total": c + h + m,
        }
        if c + h + m > 0:
            logger.info(f"  {name}: {c} critical, {h} high, {m} medium exploitable")

    now = datetime.utcnow()
    return {
        "week_date": now.strftime("%Y-%m-%d"),
        "week_number": now.isocalendar()[1],
        "metrics": {
            "current_total": total_critical + total_high + total_medium,
            "severity_breakdown": {
                "critical": total_critical,
                "high": total_high,
                "medium": total_medium,
                "total": total_critical + total_high + total_medium,
            },
            "high_data_complete": True,
            "product_breakdown": product_breakdown,
            "collected_at": now.isoformat(),
        },
    }


def _save_to_history(week_data: dict) -> None:
    """Save week_data to exploitable_history.json, replacing any existing entry for the same week_date."""
    HISTORY_PATH.parent.mkdir(parents=True, exist_ok=True)

    if HISTORY_PATH.exists():
        history = json.loads(HISTORY_PATH.read_text(encoding="utf-8"))
    else:
        history = {"weeks": []}

    week_date = week_data["week_date"]
    history["weeks"] = [w for w in history["weeks"] if w["week_date"] != week_date]
    history["weeks"].append(week_data)
    HISTORY_PATH.write_text(json.dumps(history, indent=2), encoding="utf-8")
    logger.info(f"Saved exploitable history to {HISTORY_PATH}")


def main() -> None:
    """Entry point for CI and local execution."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
    )
    logger.info("=" * 60)
    logger.info("Exploitable Vulnerability Collector (AQL/CISA KEV)")
    logger.info("=" * 60)

    week_data = collect_exploitable_metrics()
    _save_to_history(week_data)

    sev = week_data["metrics"]["severity_breakdown"]
    logger.info(
        f"Done — {sev['total']} exploitable findings "
        f"({sev['critical']} critical, {sev['high']} high, {sev['medium']} medium)"
    )


if __name__ == "__main__":
    main()
