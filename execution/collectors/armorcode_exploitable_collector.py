#!/usr/bin/env python3
"""
ArmorCode Exploitable Vulnerability Collector

Fetches High + Critical findings across all products in the configured
ArmorCode hierarchy (ARMORCODE_HIERARCHY env var — required GitHub secret)
and identifies 'exploitable' ones using the armorcode.aati.inthewild tag (> 0).

Products are resolved dynamically via productFilter.groups — no static baseline
file needed. New products added to the hierarchy are automatically included.

Queries are scoped per source bucket (INFRASTRUCTURE / CODE / CLOUD) so
each query covers a small slice of findings — avoiding the 10k API limit
while giving exact counts per bucket.

Saves results to .tmp/observatory/exploitable_history.json as weekly snapshots.

Usage:
    python execution/collectors/armorcode_exploitable_collector.py
"""

import json
import logging
import time
from datetime import datetime
from pathlib import Path

from dotenv import load_dotenv

from execution.core import get_logger
from execution.domain.security import BUCKET_ORDER, SOURCE_BUCKET_MAP
from execution.http_client import post
from execution.secure_config import get_config

load_dotenv()

logger = get_logger(__name__)

HISTORY_PATH = Path(".tmp/observatory/exploitable_history.json")
LATEST_PATH = Path(".tmp/observatory/exploitable_latest.json")
ARMORCODE_PAGE_LIMIT = 100  # ArmorCode hard limit: 100 pages × 100 items = 10k max

# Build bucket → [source strings] mapping from SOURCE_BUCKET_MAP
SOURCES_BY_BUCKET: dict[str, list[str]] = {}
for _src, _bucket in SOURCE_BUCKET_MAP.items():
    SOURCES_BY_BUCKET.setdefault(_bucket, []).append(_src)


def _calculate_age_days(created_at: str) -> int:
    """Calculate age in days from ISO8601 createdAt string."""
    if not created_at:
        return 0
    try:
        created = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
        return max(0, (datetime.now(tz=created.tzinfo) - created).days)
    except (ValueError, AttributeError):
        return 0


def _enrich_finding(f: dict, bucket: str) -> dict:
    """Reduce finding to display fields and add age_days + bucket."""
    cves = f.get("cve") or []
    return {
        "id": f.get("id", ""),
        "title": f.get("title", ""),
        "severity": f.get("severity", ""),
        "status": f.get("status", ""),
        "source": f.get("source", ""),
        "age_days": _calculate_age_days(f.get("createdAt", "")),
        "cve": cves[0] if cves else "",
        "bucket": bucket,
    }


def _get_headers() -> dict:
    """Build ArmorCode API auth headers."""
    api_key = get_config().get_armorcode_config().api_key
    return {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}


def _get_graphql_url() -> str:
    base_url = get_config().get_armorcode_config().base_url
    return f"{base_url.rstrip('/')}/api/graphql"


def _get_products_by_hierarchy(hierarchy: str) -> dict[str, str]:
    """
    Fetch all products in an ArmorCode hierarchy via productFilter.groups.

    This replaces the static baseline file approach — products are resolved
    dynamically so newly added products are automatically included.

    Args:
        hierarchy: ArmorCode hierarchy/program-group name (from ARMORCODE_HIERARCHY secret)

    Returns:
        Dict mapping product_name -> product_id
    """
    graphql_url = _get_graphql_url()
    headers = _get_headers()
    name_to_id: dict[str, str] = {}

    for page in range(1, 20):
        query = f"""
        {{
          products(
            page: {page}
            size: 100
            productFilter: {{ groups: ["{hierarchy}"] }}
          ) {{
            products {{ id name }}
            pageInfo {{ hasNext }}
          }}
        }}
        """
        try:
            resp = post(graphql_url, headers=headers, json={"query": query}, timeout=30)
            if resp.status_code != 200:
                logger.error(f"Products query HTTP {resp.status_code}")
                break
            data = resp.json()
            if "errors" in data:
                logger.error(f"Products query GraphQL error: {data['errors']}")
                break
            result = data.get("data", {}).get("products", {})
            for p in result.get("products", []):
                name_to_id[p["name"]] = p["id"]
            if not result.get("pageInfo", {}).get("hasNext", False):
                break
        except Exception as e:
            logger.error(f"Error fetching products page {page}: {e}")
            break

    logger.info(f"Found {len(name_to_id)} products in hierarchy '{hierarchy}'")
    return name_to_id


def _is_exploitable(tags: list) -> bool:
    """Return True if armorcode.aati.inthewild > 0.

    Note: ArmorCode's UI uses "CISA KEV: True" as the exploitability filter.
    The inthewild tag is ArmorCode AATI's in-the-wild exploitation signal and
    closely approximates CISA KEV (within ~6 findings for High severity).
    For Critical, a small number of findings have inthewild > 0 but are not
    in CISA KEV (identified via findingCategory=WORKSTATION in practice).
    """
    for tag in tags or []:
        if tag.startswith("armorcode.aati.inthewild:"):
            val = tag.split(":", 1)[1]
            if val.isdigit() and int(val) > 0:
                return True
    return False


def _get_aati_tag_names(tags: list) -> set[str]:
    """Extract unique armorcode.aati.* tag names (without values) for diagnostics."""
    return {tag.split(":", 1)[0] for tag in (tags or []) if tag.startswith("armorcode.aati.")}


def _matches_env(finding: dict, env_filter: str | None) -> bool:
    """Return True if finding matches the environment filter (or no filter set)."""
    if not env_filter:
        return True
    env_obj = finding.get("environment")
    if not env_obj:
        return False
    env_name = env_obj.get("name", "") if isinstance(env_obj, dict) else str(env_obj)
    return env_name.upper() == env_filter.upper()


def _fetch_exploitable_for_source(
    product_id: str,
    product_name: str,
    severity: str,
    source: str,
    bucket: str,
    env_filter: str | None = None,
) -> tuple[list[dict], bool]:
    """
    Fetch exploitable findings for one product + severity + individual source tool.

    Querying one source at a time keeps each request well under ArmorCode's
    10k result limit, giving exact exploitable counts even for high-volume
    products like Eclipse INFRASTRUCTURE/High (11k+ total findings).

    Post-fetch filters applied:
        - inthewild > 0 (exploitability signal)
        - environment filter (if configured)
        - Critical only: WORKSTATION assets excluded (matches UI CISA KEV view)
        - High: all asset types included

    Returns:
        (exploitable_findings, data_complete)
        data_complete=False only if a single source tool exceeds 10k (very unlikely)
    """
    graphql_url = _get_graphql_url()
    headers = _get_headers()
    exploitable: list[dict] = []
    page = 1
    data_complete = True
    last_has_next = False

    while page <= ARMORCODE_PAGE_LIMIT:
        query = f"""
        {{
          findings(
            page: {page}
            size: 100
            findingFilter: {{
              product: [{product_id}]
              severity: [{severity}]
              source: ["{source}"]
              status: ["OPEN", "CONFIRMED"]
            }}
          ) {{
            findings {{
              id title severity status source createdAt cve tags
              environment {{ name }}
              findingCategory
            }}
            pageInfo {{ hasNext totalElements }}
          }}
        }}
        """
        try:
            resp = post(graphql_url, headers=headers, json={"query": query}, timeout=60)
        except Exception as e:
            logger.error(f"  [{product_name}] {bucket}/{severity}/{source} page {page} request failed: {e}")
            break

        if resp.status_code == 429:
            retry_after = int(resp.headers.get("Retry-After", 60))
            logger.warning(f"  Rate limited — waiting {retry_after}s")
            time.sleep(retry_after)
            continue

        if resp.status_code != 200:
            logger.error(f"  [{product_name}] {bucket}/{severity}/{source} HTTP {resp.status_code}")
            break

        data = resp.json()
        if "errors" in data:
            err_msg = str(data["errors"])
            if "beyond 10k" in err_msg or "10000" in err_msg:
                logger.warning(f"  [{product_name}] {bucket}/{severity}/{source}: hit 10k limit at page {page}")
                data_complete = False
            else:
                logger.error(f"  [{product_name}] {bucket}/{severity}/{source} GraphQL error: {data['errors']}")
            break

        findings_data = data.get("data", {}).get("findings", {})
        page_findings = findings_data.get("findings", [])
        page_info = findings_data.get("pageInfo", {})

        for f in page_findings:
            tags = f.get("tags") or []
            if not _is_exploitable(tags):
                continue
            if not _matches_env(f, env_filter):
                continue
            # UI filter uses "CISA KEV: True" (not inthewild) for both severities.
            # For Critical: the non-CISA-KEV findings observed are all WORKSTATION
            # category — excluding them brings critical count in line with UI (11 Eclipse).
            # For High: inthewild ≈ CISA KEV.
            # TODO: replace with armorcode.aati.cisa_kev tag when confirmed.
            if severity == "Critical":
                category = (f.get("findingCategory") or "").upper()
                if category == "WORKSTATION":
                    continue
            exploitable.append(f)

        if page == 1:
            total = page_info.get("totalElements", 0)
            if total > 0:
                logger.debug(f"  [{product_name}] {source}/{severity}: {total} total findings")

        last_has_next = page_info.get("hasNext", False)
        if not last_has_next:
            break

        page += 1

    if page > ARMORCODE_PAGE_LIMIT and last_has_next:
        logger.warning(f"  [{product_name}] {source}/{severity}: hit {ARMORCODE_PAGE_LIMIT}-page limit — data partial")
        data_complete = False

    return exploitable, data_complete


def _fetch_exploitable_for_bucket(
    product_id: str,
    product_name: str,
    severity: str,
    sources: list[str],
    bucket: str,
    env_filter: str | None = None,
) -> tuple[list[dict], bool]:
    """
    Fetch exploitable findings for one product + severity + source bucket.

    Queries each source tool individually (not all at once) so that each
    request covers only a slice of findings — avoiding the 10k API limit
    even for high-volume buckets like Eclipse INFRASTRUCTURE/High (11k+ total).

    Returns:
        (exploitable_findings, data_complete)
        data_complete=False only if any individual source exceeds 10k (very unlikely)
    """
    all_exploitable: list[dict] = []
    all_complete = True

    for source in sources:
        findings, complete = _fetch_exploitable_for_source(
            product_id, product_name, severity, source, bucket, env_filter
        )
        all_exploitable.extend(findings)
        if not complete:
            all_complete = False

    if all_exploitable or logger.isEnabledFor(logging.DEBUG):
        logger.info(
            f"  [{product_name}] {bucket}/{severity}: {len(all_exploitable)} exploitable"
            f" (across {len(sources)} source tools)"
        )

    return all_exploitable, all_complete


def _collect_product_exploitables(product_name: str, product_id: str, env_filter: str | None = None) -> dict:
    """
    Collect exploitable findings for one product, broken down by source bucket.

    Queries each bucket (INFRASTRUCTURE / CODE / CLOUD) separately so each
    query targets a small subset of findings — avoiding the 10k API limit and
    giving exact counts per bucket.

    Returns:
        Dict with critical, high, high_data_complete, buckets, top_cves
    """
    logger.info(f"Collecting exploitable findings for: {product_name}")

    buckets: dict[str, dict[str, int]] = {b: {"critical": 0, "high": 0} for b in BUCKET_ORDER}
    all_cves: set[str] = set()
    all_aati_tag_names: set[str] = set()
    findings_list: list[dict] = []
    total_critical = 0
    total_high = 0
    high_complete = True

    for bucket, sources in SOURCES_BY_BUCKET.items():
        if bucket == "Other":
            continue  # Skip unknown sources

        # Critical findings for this bucket (should always be small, < 10k)
        crit_findings, _ = _fetch_exploitable_for_bucket(
            product_id, product_name, "Critical", sources, bucket, env_filter
        )
        # High findings for this bucket (per-bucket scope keeps this manageable)
        high_findings, h_complete = _fetch_exploitable_for_bucket(
            product_id, product_name, "High", sources, bucket, env_filter
        )

        if not h_complete:
            high_complete = False

        crit_count = len(crit_findings)
        high_count = len(high_findings)

        if crit_count + high_count > 0:
            buckets[bucket]["critical"] = crit_count
            buckets[bucket]["high"] = high_count

        total_critical += crit_count
        total_high += high_count

        # Collect unique CVEs and AATI tag names from all exploitable findings
        for f in crit_findings + high_findings:
            for cve in f.get("cve") or []:
                if cve:
                    all_cves.add(cve)
            all_aati_tag_names |= _get_aati_tag_names(f.get("tags") or [])
            findings_list.append(_enrich_finding(f, bucket))

    logger.info(
        f"  {product_name}: {total_critical} exploitable critical, "
        f"{total_high} exploitable high" + ("" if high_complete else " [HIGH data partial - 10k limit]")
    )
    # Log unique AATI tag names to help identify CISA KEV tag (e.g. armorcode.aati.cisa_kev)
    if all_aati_tag_names:
        logger.debug(f"  {product_name} AATI tags observed: {sorted(all_aati_tag_names)}")

    return {
        "critical": total_critical,
        "high": total_high,
        "high_data_complete": high_complete,
        "total": total_critical + total_high,
        "buckets": {k: v for k, v in buckets.items() if v["critical"] + v["high"] > 0},
        "top_cves": sorted(all_cves),
        "findings": findings_list,
    }


def collect_exploitable_metrics() -> dict:
    """
    Collect exploitable findings across all baseline products.

    Returns:
        Metrics dict ready to append to exploitable_history.json
    """
    hierarchy = get_config().get_optional_env("ARMORCODE_HIERARCHY")
    if not hierarchy:
        raise RuntimeError(
            "ARMORCODE_HIERARCHY env var not set. "
            "Add it as a GitHub secret and to your local .env file. "
            "Value: the ArmorCode hierarchy/program-group to scope the collection to."
        )
    logger.info("Hierarchy: configured (value kept out of logs for security)")
    name_to_id = _get_products_by_hierarchy(hierarchy)

    env_filter = get_config().get_optional_env("ARMORCODE_ENVIRONMENT")
    if env_filter:
        logger.info(f"Environment filter: {env_filter} (server-only)")
    else:
        logger.info("No environment filter — including all asset types")

    product_breakdown: dict[str, dict] = {}
    product_findings: dict[str, list[dict]] = {}
    total_critical = 0
    total_high = 0
    any_incomplete = False

    for name, product_id in sorted(name_to_id.items()):
        result = _collect_product_exploitables(name, product_id, env_filter)
        product_findings[name] = result.pop("findings", [])  # separate from history data
        product_breakdown[name] = result
        total_critical += result["critical"]
        total_high += result["high"]
        if not result["high_data_complete"]:
            any_incomplete = True

    _save_latest_snapshot(product_findings)

    now = datetime.utcnow()
    return {
        "week_date": now.strftime("%Y-%m-%d"),
        "week_number": now.isocalendar()[1],
        "metrics": {
            "current_total": total_critical + total_high,
            "severity_breakdown": {
                "critical": total_critical,
                "high": total_high,
                "total": total_critical + total_high,
            },
            "high_data_complete": not any_incomplete,
            "product_breakdown": product_breakdown,
            "collected_at": now.isoformat(),
        },
    }


def _save_latest_snapshot(product_findings: dict[str, list[dict]]) -> None:
    """Write per-finding details to exploitable_latest.json (overwritten each run)."""
    LATEST_PATH.parent.mkdir(parents=True, exist_ok=True)
    snapshot = {
        "collected_at": datetime.utcnow().isoformat(),
        "products": product_findings,
    }
    LATEST_PATH.write_text(json.dumps(snapshot, indent=2), encoding="utf-8")
    total = sum(len(v) for v in product_findings.values())
    logger.info(f"Saved {total} findings to {LATEST_PATH}")


def _save_to_history(week_data: dict) -> None:
    """Append week_data to exploitable_history.json."""
    HISTORY_PATH.parent.mkdir(parents=True, exist_ok=True)

    if HISTORY_PATH.exists():
        history = json.loads(HISTORY_PATH.read_text(encoding="utf-8"))
    else:
        history = {"weeks": []}

    history["weeks"].append(week_data)
    HISTORY_PATH.write_text(json.dumps(history, indent=2), encoding="utf-8")
    logger.info(f"Saved exploitable history to {HISTORY_PATH}")


def main() -> None:
    """Entry point for CI and local execution."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
    )
    logger.info("=" * 60)
    logger.info("Exploitable Vulnerability Collector")
    logger.info("=" * 60)

    week_data = collect_exploitable_metrics()
    _save_to_history(week_data)

    total = week_data["metrics"]["severity_breakdown"]["total"]
    crit = week_data["metrics"]["severity_breakdown"]["critical"]
    high = week_data["metrics"]["severity_breakdown"]["high"]
    logger.info(f"Done — {total} exploitable findings ({crit} critical, {high} high)")


if __name__ == "__main__":
    main()
