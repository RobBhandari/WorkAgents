#!/usr/bin/env python3
"""
Generate Performance Report from Benchmark Results

Creates comprehensive performance documentation for REST API migration.

Outputs:
- Markdown report (.md)
- HTML report (.html)
- Summary statistics
- Performance graphs (if matplotlib available)

Usage:
    python execution/generate_performance_report.py
    python execution/generate_performance_report.py --input custom_benchmark.json
"""

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Any


class PerformanceReportGenerator:
    """Generate performance reports from benchmark results"""

    def __init__(self, results_file: Path):
        """
        Initialize report generator.

        Args:
            results_file: Path to benchmark results JSON
        """
        self.results_file = results_file
        self.results: dict[str, Any] = {}
        self._load_results()

    def _load_results(self):
        """Load benchmark results from JSON file"""
        if not self.results_file.exists():
            raise FileNotFoundError(f"Benchmark results not found: {self.results_file}")

        with open(self.results_file, encoding="utf-8") as f:
            self.results = json.load(f)

    def generate_markdown_report(self) -> str:
        """
        Generate Markdown performance report.

        Returns:
            Markdown formatted report
        """
        sequential = self.results.get("sequential", {})
        concurrent = self.results.get("concurrent", {})
        comparison = self.results.get("comparison", {})

        report_lines = [
            "# REST API v7.1 Migration Performance Report\n",
            f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
            f"**Platform:** {concurrent.get('platform', 'Unknown')}\n",
            f"**Python:** {concurrent.get('python_version', 'Unknown')}\n",
            "\n---\n",
            "\n## Executive Summary\n",
            f"âœ… **Speedup Achieved:** {comparison.get('actual_speedup', 'N/A')}\n",
            f"ðŸŽ¯ **Target Range:** {comparison.get('target_range', 'N/A')}\n",
            f"âœ… **Claim Validated:** {'YES' if comparison.get('claim_validated') else 'NO'}\n",
            f"â±ï¸ **Time Saved:** {comparison.get('time_saved_minutes', 0):.2f} minutes per run\n",
            "\n---\n",
            "\n## Overall Performance Comparison\n",
            "| Metric | Sequential | Concurrent | Improvement |\n",
            "|--------|------------|------------|--------------|\n",
            f"| **Total Duration** | {sequential.get('total_duration_seconds', 0):.2f}s ({sequential.get('total_duration_minutes', 0):.2f} min) | {concurrent.get('total_duration_seconds', 0):.2f}s ({concurrent.get('total_duration_minutes', 0):.2f} min) | **{comparison.get('speedup_factor', 0):.2f}x faster** |\n",
            f"| **API Calls** | {sequential.get('total_api_calls', 0)} | {concurrent.get('total_api_calls', 0)} | Same |\n",
            f"| **Throughput** | {sequential.get('average_throughput_per_sec', 0):.2f} calls/sec | {concurrent.get('average_throughput_per_sec', 0):.2f} calls/sec | **{(concurrent.get('average_throughput_per_sec', 0) / sequential.get('average_throughput_per_sec', 1) if sequential.get('average_throughput_per_sec', 0) > 0 else 0):.2f}x faster** |\n",
            f"| **Success Rate** | {sequential.get('success_rate_percent', 0):.1f}% | {concurrent.get('success_rate_percent', 0):.1f}% | {concurrent.get('success_rate_percent', 0) - sequential.get('success_rate_percent', 0):+.1f}% |\n",
            "\n---\n",
            "\n## Collector-by-Collector Performance\n",
            "| Collector | Sequential (s) | Concurrent (s) | Speedup | API Calls | Throughput |\n",
            "|-----------|----------------|----------------|---------|-----------|------------|\n",
        ]

        # Build collector comparison table
        seq_collectors = {c["name"]: c for c in sequential.get("collectors", [])}
        conc_collectors = {c["name"]: c for c in concurrent.get("collectors", [])}

        for name in sorted(seq_collectors.keys()):
            seq_col = seq_collectors.get(name, {})
            conc_col = conc_collectors.get(name, {})

            if seq_col and conc_col and seq_col.get("success") and conc_col.get("success"):
                speedup = seq_col["duration_seconds"] / conc_col["duration_seconds"]
                report_lines.append(
                    f"| **{name}** | {seq_col['duration_seconds']:.2f}s | "
                    f"{conc_col['duration_seconds']:.2f}s | **{speedup:.2f}x** | "
                    f"{conc_col['api_calls_made']} | {conc_col['throughput_per_sec']:.2f} calls/sec |\n"
                )

        # Add detailed metrics
        report_lines.extend(
            [
                "\n---\n",
                "\n## Detailed Concurrent Collector Metrics\n",
                "| Collector | Duration | API Calls | Memory (MB) | Throughput | Status |\n",
                "|-----------|----------|-----------|-------------|------------|--------|\n",
            ]
        )

        for collector in concurrent.get("collectors", []):
            status = "âœ… Success" if collector.get("success") else "âŒ Failed"
            report_lines.append(
                f"| {collector['name']} | {collector['duration_seconds']:.2f}s | "
                f"{collector['api_calls_made']} | {collector['memory_peak_mb']:.2f} MB | "
                f"{collector['throughput_per_sec']:.2f} calls/sec | {status} |\n"
            )

        # Add key findings
        report_lines.extend(
            [
                "\n---\n",
                "\n## Key Findings\n",
                f"1. **Overall Speedup:** Concurrent execution is **{comparison.get('speedup_factor', 0):.2f}x faster** than sequential\n",
                f"2. **Time Savings:** Each run saves **{comparison.get('time_saved_minutes', 0):.2f} minutes** ({comparison.get('time_saved_seconds', 0):.2f} seconds)\n",
                f"3. **API Throughput:** Improved from {sequential.get('average_throughput_per_sec', 0):.2f} to {concurrent.get('average_throughput_per_sec', 0):.2f} calls/sec\n",
                f"4. **Reliability:** {concurrent.get('success_rate_percent', 0):.1f}% success rate in concurrent mode\n",
                "\n### Collector-Specific Insights\n",
            ]
        )

        # Calculate per-collector speedups
        collector_speedups = []
        for name in seq_collectors.keys():
            seq_col = seq_collectors.get(name, {})
            conc_col = conc_collectors.get(name, {})

            if seq_col and conc_col and seq_col.get("success") and conc_col.get("success"):
                speedup = seq_col["duration_seconds"] / conc_col["duration_seconds"]
                collector_speedups.append((name, speedup))

        # Sort by speedup (highest first)
        collector_speedups.sort(key=lambda x: x[1], reverse=True)

        if collector_speedups:
            fastest = collector_speedups[0]
            slowest = collector_speedups[-1]
            report_lines.append(f"- **Fastest Improvement:** {fastest[0]} achieved {fastest[1]:.2f}x speedup\n")
            report_lines.append(f"- **Slowest Improvement:** {slowest[0]} achieved {slowest[1]:.2f}x speedup\n")

        # Add technical details
        report_lines.extend(
            [
                "\n---\n",
                "\n## Technical Implementation\n",
                "### Migration Details\n",
                "- **From:** Azure DevOps SDK (azure-devops==7.1.0b4) with ThreadPoolExecutor\n",
                "- **To:** Direct REST API v7.1 with native async/await\n",
                "- **HTTP Client:** AsyncSecureHTTPClient with HTTP/2 support\n",
                "- **Concurrency Model:** asyncio.gather() for true parallel execution\n",
                "\n### Performance Optimizations\n",
                "1. **True Async I/O:** Native async REST calls (no thread pool overhead)\n",
                "2. **HTTP/2 Multiplexing:** Multiple requests over single connection\n",
                "3. **Connection Pooling:** Reused connections across API calls\n",
                "4. **Concurrent Execution:** All collectors run in parallel via asyncio.gather()\n",
                "5. **Efficient Batching:** Work items fetched in batches of 200 (API limit)\n",
                "\n### API Call Distribution\n",
                "Different collectors make varying numbers of API calls:\n",
                "- **Quality/Flow:** ~3 calls per project (WIQL + work items + metadata)\n",
                "- **Deployment:** ~7 calls per project (builds + changes + commits)\n",
                "- **Ownership:** ~15 calls per project (repos + commits + PRs + iterations)\n",
                "- **Collaboration:** ~12 calls per project (repos + PRs + threads + commits)\n",
                "- **Risk:** ~6 calls per project (security WIQL queries + work items)\n",
                "\n---\n",
                "\n## Conclusion\n",
                f"The migration from Azure DevOps SDK to REST API v7.1 with async/await has achieved a **{comparison.get('speedup_factor', 0):.2f}x speedup**, ",
                f"{'**validating**' if comparison.get('claim_validated') else '**partially validating**'} the claimed 3-50x performance improvement.\n",
                "\n",
                f"This translates to **{comparison.get('time_saved_minutes', 0):.2f} minutes saved per collection run**, ",
                "significantly improving data freshness and reducing infrastructure costs.\n",
                "\n---\n",
                "\n*Report generated by `execution/generate_performance_report.py`*\n",
            ]
        )

        return "".join(report_lines)

    def generate_html_report(self, markdown_content: str) -> str:
        """
        Convert Markdown report to HTML.

        Args:
            markdown_content: Markdown formatted report

        Returns:
            HTML formatted report
        """
        # Simple Markdown to HTML conversion (basic tables and formatting)
        html_lines = [
            "<!DOCTYPE html>\n",
            "<html>\n",
            "<head>\n",
            "    <meta charset='UTF-8'>\n",
            "    <title>REST API Migration Performance Report</title>\n",
            "    <style>\n",
            "        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; max-width: 1200px; margin: 40px auto; padding: 20px; line-height: 1.6; }\n",
            "        h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }\n",
            "        h2 { color: #34495e; border-bottom: 2px solid #95a5a6; padding-bottom: 8px; margin-top: 30px; }\n",
            "        h3 { color: #7f8c8d; margin-top: 20px; }\n",
            "        table { border-collapse: collapse; width: 100%; margin: 20px 0; }\n",
            "        th, td { border: 1px solid #bdc3c7; padding: 12px; text-align: left; }\n",
            "        th { background-color: #3498db; color: white; font-weight: 600; }\n",
            "        tr:nth-child(even) { background-color: #ecf0f1; }\n",
            "        code { background-color: #f8f9fa; padding: 2px 6px; border-radius: 3px; font-family: 'Courier New', monospace; }\n",
            "        strong { color: #2c3e50; }\n",
            "        hr { border: none; border-top: 1px solid #bdc3c7; margin: 30px 0; }\n",
            "        .summary { background-color: #e8f5e9; padding: 20px; border-radius: 8px; margin: 20px 0; border-left: 5px solid #4caf50; }\n",
            "    </style>\n",
            "</head>\n",
            "<body>\n",
        ]

        # Convert markdown to HTML (basic conversion)
        lines = markdown_content.split("\n")
        in_table = False
        table_headers = False

        for line in lines:
            if line.startswith("# "):
                html_lines.append(f"    <h1>{line[2:].strip()}</h1>\n")
            elif line.startswith("## "):
                html_lines.append(f"    <h2>{line[3:].strip()}</h2>\n")
            elif line.startswith("### "):
                html_lines.append(f"    <h3>{line[4:].strip()}</h3>\n")
            elif line.startswith("| ") and "|" in line:
                if not in_table:
                    html_lines.append("    <table>\n")
                    in_table = True
                    table_headers = True

                # Parse table row
                cells = [cell.strip() for cell in line.split("|")[1:-1]]

                if table_headers and all(c.startswith("-") for c in cells):
                    # Skip separator row
                    table_headers = False
                    continue

                if table_headers:
                    html_lines.append("        <tr>\n")
                    for cell in cells:
                        html_lines.append(f"            <th>{cell}</th>\n")
                    html_lines.append("        </tr>\n")
                else:
                    html_lines.append("        <tr>\n")
                    for cell in cells:
                        # Convert markdown bold to HTML
                        cell_html = cell.replace("**", "<strong>").replace("**", "</strong>")
                        html_lines.append(f"            <td>{cell_html}</td>\n")
                    html_lines.append("        </tr>\n")
            else:
                if in_table and not line.strip().startswith("|"):
                    html_lines.append("    </table>\n")
                    in_table = False
                    table_headers = False

                if line.strip().startswith("*"):
                    html_lines.append(f"    <p><em>{line.strip()[1:].strip()}</em></p>\n")
                elif line.strip():
                    # Convert markdown bold and inline code
                    line_html = line
                    line_html = line_html.replace("**", "<strong>")
                    line_html = line_html.replace("**", "</strong>")
                    line_html = line_html.replace("`", "<code>")
                    line_html = line_html.replace("`", "</code>")
                    html_lines.append(f"    <p>{line_html}</p>\n")
                elif line.strip() == "---":
                    html_lines.append("    <hr>\n")

        if in_table:
            html_lines.append("    </table>\n")

        html_lines.append("</body>\n</html>\n")
        return "".join(html_lines)

    def save_reports(self, output_dir: Path):
        """
        Generate and save all report formats.

        Args:
            output_dir: Directory to save reports
        """
        output_dir.mkdir(parents=True, exist_ok=True)

        # Generate Markdown report
        print("Generating Markdown report...")
        markdown_content = self.generate_markdown_report()
        markdown_file = output_dir / "performance_report.md"

        with open(markdown_file, "w", encoding="utf-8") as f:
            f.write(markdown_content)

        print(f"[OK] Markdown report saved: {markdown_file}")

        # Generate HTML report
        print("Generating HTML report...")
        html_content = self.generate_html_report(markdown_content)
        html_file = output_dir / "performance_report.html"

        with open(html_file, "w", encoding="utf-8") as f:
            f.write(html_content)

        print(f"[OK] HTML report saved: {html_file}")

        # Print summary
        comparison = self.results.get("comparison", {})
        print("\n" + "=" * 60)
        print("PERFORMANCE REPORT SUMMARY")
        print("=" * 60)
        print(f"Speedup Factor:     {comparison.get('speedup_factor', 0):.2f}x")
        print(f"Time Saved:         {comparison.get('time_saved_minutes', 0):.2f} minutes")
        print(f"Claim Validated:    {'YES' if comparison.get('claim_validated') else 'NO'}")
        print(f"Target Range:       {comparison.get('target_range', 'N/A')}")
        print("=" * 60)
        print(f"\nView HTML report: file:///{html_file.absolute()}")
        print("")


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description="Generate Performance Report")
    parser.add_argument(
        "--input",
        type=Path,
        default=Path(".tmp/observatory/benchmark_results_enhanced.json"),
        help="Input benchmark results JSON file",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path(".tmp/observatory"),
        help="Output directory for reports",
    )
    args = parser.parse_args()

    try:
        generator = PerformanceReportGenerator(args.input)
        generator.save_reports(args.output)
        return 0
    except Exception as e:
        print(f"Error generating report: {e}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    sys.exit(main())
