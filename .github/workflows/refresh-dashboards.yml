name: Refresh Observatory Dashboards

on:
  # Run every day at 6 AM UTC (adjust timezone as needed)
  schedule:
    - cron: '0 6 * * *'

  # Allow manual trigger from GitHub UI
  workflow_dispatch:

permissions:
  contents: write  # Allow workflow to commit and push history files

jobs:
  # Job 1: Discover Projects (MUST run first - other collectors depend on it)
  discover-projects:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create .env file from secrets
        run: |
          echo "AZURE_DEVOPS_ORG_URL=${{ secrets.AZURE_DEVOPS_ORG_URL }}" >> .env
          echo "AZURE_DEVOPS_PAT=${{ secrets.AZURE_DEVOPS_PAT }}" >> .env
          echo "ADO_ORGANIZATION_URL=${{ secrets.AZURE_DEVOPS_ORG_URL }}" >> .env
          echo "ADO_PAT=${{ secrets.AZURE_DEVOPS_PAT }}" >> .env

      - name: Create baseline files from secrets
        run: |
          mkdir -p data
          echo "üìÇ Creating baseline files from GitHub secrets..."

          # Main ADO baseline
          echo '${{ secrets.BASELINE_ADO_MAIN }}' > data/baseline.json

          # Project-specific baselines (if configured)
          if [ -n "${{ secrets.BASELINE_ADO_PROJECTS }}" ]; then
            echo '${{ secrets.BASELINE_ADO_PROJECTS }}' | jq -r '.projects[]? | .baseline_file as $file | .baseline_data | "echo '\''\(. | @json)'\'' > data/\($file)"' | bash || true
          fi

          echo "‚úÖ Baseline files created"
          ls -la data/*.json 2>/dev/null || echo "‚ö†Ô∏è No baseline files created (secrets may be empty)"

      - name: Discover Projects
        run: |
          export PYTHONPATH="${GITHUB_WORKSPACE}:${PYTHONPATH}"
          python execution/discover_projects.py

      - name: Upload project discovery artifact
        uses: actions/upload-artifact@v4
        with:
          name: project-discovery
          path: .tmp/observatory/ado_structure.json
          retention-days: 1

  # Job 2: Collect all other metrics in parallel (depends on project discovery)
  collect-metrics:
    needs: discover-projects
    runs-on: ubuntu-latest
    strategy:
      matrix:
        collector:
          - { name: 'Quality Metrics', script: 'collectors/ado_quality_metrics.py', outputs: 'quality_history.json' }
          - { name: 'Flow Metrics', script: 'collectors/ado_flow_metrics.py', outputs: 'flow_history.json' }
          - { name: 'Ownership Metrics', script: 'collectors/ado_ownership_metrics.py', outputs: 'ownership_history.json' }
          - { name: 'Risk Metrics', script: 'collectors/ado_risk_metrics.py', outputs: 'risk_history.json' }
          - { name: 'Deployment Metrics', script: 'collectors/ado_deployment_metrics.py', outputs: 'deployment_history.json' }
          - { name: 'Collaboration Metrics', script: 'collectors/ado_collaboration_metrics.py', outputs: 'collaboration_history.json' }
          - { name: 'Security Metrics', script: 'armorcode_enhanced_metrics.py', outputs: 'security_history.json' }
      fail-fast: false  # Continue even if one fails
      max-parallel: 7   # Run all 7 collectors at once

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create .env file from secrets
        run: |
          echo "AZURE_DEVOPS_ORG_URL=${{ secrets.AZURE_DEVOPS_ORG_URL }}" >> .env
          echo "AZURE_DEVOPS_PAT=${{ secrets.AZURE_DEVOPS_PAT }}" >> .env
          echo "ADO_ORGANIZATION_URL=${{ secrets.AZURE_DEVOPS_ORG_URL }}" >> .env
          echo "ADO_PAT=${{ secrets.AZURE_DEVOPS_PAT }}" >> .env
          echo "ARMORCODE_BASE_URL=${{ secrets.ARMORCODE_BASE_URL }}" >> .env
          echo "ARMORCODE_API_KEY=${{ secrets.ARMORCODE_API_KEY }}" >> .env

      - name: Create baseline files from secrets
        run: |
          mkdir -p data
          echo "üìÇ Creating baseline files from GitHub secrets..."

          # Main ADO baseline
          echo '${{ secrets.BASELINE_ADO_MAIN }}' > data/baseline.json

          # ArmorCode baseline
          echo '${{ secrets.BASELINE_ARMORCODE }}' > data/armorcode_baseline.json

          # Project-specific baselines (if configured)
          if [ -n "${{ secrets.BASELINE_ADO_PROJECTS }}" ]; then
            echo '${{ secrets.BASELINE_ADO_PROJECTS }}' | jq -r '.projects[]? | .baseline_file as $file | .baseline_data | "echo '\''\(. | @json)'\'' > data/\($file)"' | bash || true
          fi

          echo "‚úÖ Baseline files created"
          ls -la data/*.json 2>/dev/null || echo "‚ö†Ô∏è No baseline files created (secrets may be empty)"

      - name: Download project discovery artifact
        uses: actions/download-artifact@v4
        with:
          name: project-discovery
          path: .tmp/observatory/

      - name: Verify project discovery file
        run: |
          if [ -f .tmp/observatory/ado_structure.json ]; then
            echo "‚úÖ Project discovery file found"
            ls -la .tmp/observatory/ado_structure.json
          else
            echo "‚ùå Project discovery file not found!"
            exit 1
          fi

      - name: De-genericize history files (reverse Product A ‚Üí Real Names)
        run: |
          export PYTHONPATH="${GITHUB_WORKSPACE}:${PYTHONPATH}"
          echo "üîì Converting Product A, B, C back to real product names..."
          python scripts/de_genericize_history_files.py || echo "‚ö†Ô∏è  No history files to de-genericize (first run or not found)"
          echo "‚úÖ History files now have REAL names - collectors will append with real names"

      - name: Collect ${{ matrix.collector.name }}
        id: collect
        run: |
          export PYTHONPATH="${GITHUB_WORKSPACE}:${PYTHONPATH}"
          python execution/${{ matrix.collector.script }}

      - name: Upload metrics artifacts
        uses: actions/upload-artifact@v4
        with:
          name: metrics-${{ matrix.collector.name }}
          path: .tmp/observatory/${{ matrix.collector.outputs }}
          retention-days: 1
        if: steps.collect.outcome == 'success'

  # Job 3: Generate all dashboards in parallel
  generate-dashboards:
    needs: collect-metrics
    if: always()  # Run even if collect-metrics fails
    runs-on: ubuntu-latest
    strategy:
      matrix:
        dashboard:
          - { name: 'Quality', script: 'quality.py' }
          - { name: 'Flow', script: 'flow.py' }
          - { name: 'Ownership', script: 'ownership.py' }
          - { name: 'Risk', script: 'risk.py' }
          - { name: 'Deployment', script: 'deployment.py' }
          - { name: 'Collaboration', script: 'collaboration.py' }
          - { name: 'Security', script: 'security_enhanced.py' }
          - { name: 'Targets', script: 'targets.py' }
          - { name: 'AI Usage', script: 'ai.py' }
          - { name: 'Trends', script: 'trends.py' }
          - { name: 'Executive', script: 'executive.py' }
      fail-fast: false
      max-parallel: 11

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create .env file from secrets
        run: |
          echo "ARMORCODE_BASE_URL=${{ secrets.ARMORCODE_BASE_URL }}" >> .env
          echo "ARMORCODE_API_KEY=${{ secrets.ARMORCODE_API_KEY }}" >> .env

      - name: Create baseline files from secrets
        run: |
          mkdir -p data
          echo "üìÇ Creating baseline files from GitHub secrets..."

          # Main ADO baseline
          echo '${{ secrets.BASELINE_ADO_MAIN }}' > data/baseline.json

          # ArmorCode baseline
          echo '${{ secrets.BASELINE_ARMORCODE }}' > data/armorcode_baseline.json

          # Project-specific baselines (if configured)
          if [ -n "${{ secrets.BASELINE_ADO_PROJECTS }}" ]; then
            echo '${{ secrets.BASELINE_ADO_PROJECTS }}' | jq -r '.projects[]? | .baseline_file as $file | .baseline_data | "echo '\''\(. | @json)'\'' > data/\($file)"' | bash || true
          fi

          echo "‚úÖ Baseline files created"
          ls -la data/*.json 2>/dev/null || echo "‚ö†Ô∏è No baseline files created (secrets may be empty)"

      - name: Download ALL metrics artifacts (REQUIRED - no fallback!)
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
        continue-on-error: false

      - name: Verify fresh metrics available
        run: |
          echo "üîç Verifying fresh metrics artifacts are available..."
          if [ ! -d "artifacts" ] || [ -z "$(ls -A artifacts 2>/dev/null)" ]; then
            echo "‚ùå ERROR: No fresh metrics artifacts found!"
            echo "Cannot generate dashboards with real product names without fresh data."
            echo "This prevents falling back to genericized data from git."
            exit 1
          fi

          echo "‚úÖ Fresh metrics artifacts found:"
          ls -la artifacts/

      - name: Use ONLY fresh metrics (ignore genericized git history)
        run: |
          # Create output directory
          mkdir -p .tmp/observatory

          # Copy ONLY fresh artifacts with REAL product names
          echo "üì¶ Using fresh metrics with REAL product names..."
          find artifacts -name "*.json" -exec cp -v {} .tmp/observatory/ \;

          echo ""
          echo "‚úÖ Fresh history files ready for dashboard generation:"
          ls -lh .tmp/observatory/*.json
          echo ""
          echo "üîí These contain REAL product names (de-genericized + fresh appended data)"

      - name: Verify metrics data available
        id: verify
        run: |
          # Check for the specific history file this dashboard needs
          DASHBOARD_NAME="${{ matrix.dashboard.name }}"

          # Map dashboard names to their required history files
          case "$DASHBOARD_NAME" in
            "Quality")
              REQUIRED_FILE="quality_history.json"
              ;;
            "Flow")
              REQUIRED_FILE="flow_history.json"
              ;;
            "Ownership")
              REQUIRED_FILE="ownership_history.json"
              ;;
            "Risk")
              REQUIRED_FILE="risk_history.json"
              ;;
            "Deployment")
              REQUIRED_FILE="deployment_history.json"
              ;;
            "Collaboration")
              REQUIRED_FILE="collaboration_history.json"
              ;;
            "Security")
              REQUIRED_FILE="security_history.json"
              ;;
            "Targets"|"Executive")
              # These dashboards aggregate all history files
              REQUIRED_FILE="quality_history.json"  # Just check if any exists
              ;;
            "AI Usage")
              # AI Usage doesn't require history files
              echo "‚úÖ AI Usage dashboard doesn't require history files"
              exit 0
              ;;
            *)
              echo "‚ö†Ô∏è  Unknown dashboard type"
              exit 0
              ;;
          esac

          if [ -f ".tmp/observatory/$REQUIRED_FILE" ]; then
            echo "‚úÖ Required history file found: $REQUIRED_FILE"
            ls -lh ".tmp/observatory/$REQUIRED_FILE"
          else
            echo "‚ùå Required history file not found: $REQUIRED_FILE"
            echo "‚ö†Ô∏è  Dashboard generation may fail or show incomplete data"
            exit 1
          fi

      - name: Generate ${{ matrix.dashboard.name }} Dashboard
        if: steps.verify.outcome == 'success' || matrix.dashboard.name == 'AI Usage'
        run: |
          export PYTHONPATH="${GITHUB_WORKSPACE}:${PYTHONPATH}"
          python execution/dashboards/${{ matrix.dashboard.script }}

      - name: Upload dashboard artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dashboard-${{ matrix.dashboard.name }}
          path: .tmp/observatory/dashboards/
          retention-days: 1
        if: success()

  # Job 4: Deploy to Azure Static Web Apps
  deploy-to-azure:
    needs: generate-dashboards
    if: always()  # Run even if some dashboards fail
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all dashboard artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: dashboard-*
          path: dashboard-artifacts/
          merge-multiple: true

      - name: Download fresh metrics artifacts (REQUIRED for git commit)
        uses: actions/download-artifact@v4
        with:
          pattern: metrics-*
          path: metrics-artifacts/
        continue-on-error: false

      - name: Verify and merge fresh metrics
        run: |
          mkdir -p .tmp/observatory

          # Verify fresh metrics are available
          if [ ! -d "metrics-artifacts" ] || [ -z "$(find metrics-artifacts -name '*_history.json' 2>/dev/null)" ]; then
            echo "‚ùå ERROR: No fresh metrics artifacts found for git commit!"
            echo "Cannot commit history without fresh data."
            exit 1
          fi

          # Copy fresh history files with REAL product names
          echo "üì¶ Merging fresh metrics with REAL product names..."
          find metrics-artifacts -name "*_history.json" -exec cp -v {} .tmp/observatory/ \;

          echo ""
          echo "‚úÖ Fresh history files ready for genericization and commit:"
          ls -lh .tmp/observatory/*_history.json

      - name: Prepare deployment directory
        run: |
          mkdir -p .tmp/observatory/dashboards

          # Copy all dashboard HTML files
          if [ -d "dashboard-artifacts" ]; then
            cp -r dashboard-artifacts/* .tmp/observatory/dashboards/ 2>/dev/null || true
            echo "‚úÖ Dashboards prepared for deployment"
            ls -la .tmp/observatory/dashboards/
          else
            echo "‚ö†Ô∏è  No dashboards found, deployment will be empty"
          fi

      - name: Deploy to Azure Static Web Apps
        uses: Azure/static-web-apps-deploy@v1
        with:
          azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN }}
          repo_token: ${{ secrets.GITHUB_TOKEN }}
          action: "upload"
          app_location: ".tmp/observatory/dashboards"
          skip_app_build: true
          skip_api_build: true

      - name: Deployment Complete
        run: |
          echo "‚úÖ Dashboards deployed successfully to Azure Static Web Apps!"
          echo "üîí Access is now protected with Azure AD authentication"
          echo "üåê Your team can access at: https://${{ secrets.AZURE_STATIC_WEB_APP_NAME }}.azurestaticapps.net/"

      - name: Genericize History Files Before Commit
        run: |
          export PYTHONPATH="${GITHUB_WORKSPACE}:${PYTHONPATH}"
          echo "üîí Genericizing product names in history files for public repo..."
          python scripts/genericize_history_files.py || echo "Warning: Genericization script had issues, but continuing..."

      - name: Validate History Files Before Commit
        run: |
          echo "üîç Validating history files before committing..."
          python -c "
          import json
          import sys
          from pathlib import Path

          history_files = list(Path('.tmp/observatory').glob('*_history.json'))
          valid_files = []
          invalid_files = []

          for file in history_files:
              try:
                  with open(file, 'r') as f:
                      data = json.load(f)

                  # Check basic structure
                  if not isinstance(data, dict) or 'weeks' not in data:
                      invalid_files.append(f'{file.name}: Invalid structure')
                      continue

                  # Check if weeks array has data
                  if not data['weeks']:
                      invalid_files.append(f'{file.name}: Empty weeks array')
                      continue

                  valid_files.append(file.name)
                  print(f'‚úì {file.name}: Valid ({len(data[\"weeks\"])} weeks)')
              except Exception as e:
                  invalid_files.append(f'{file.name}: {e}')

          if invalid_files:
              print('\n‚ö†Ô∏è  Invalid history files found:')
              for f in invalid_files:
                  print(f'  - {f}')
              print('\nThese files will NOT be committed.')

          if not valid_files:
              print('\n‚ùå No valid history files to commit!')
              sys.exit(1)

          print(f'\n‚úÖ {len(valid_files)} valid history files ready to commit')
          "

      - name: Commit Genericized History Files to Git
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Only add valid JSON files (validation passed above)
          git add -f .tmp/observatory/*_history.json

          if ! git diff --staged --quiet; then
            echo "üìù Committing genericized history files..."
            git commit -m "Update metrics history (genericized) - $(date +'%Y-%m-%d')"
            git push
            echo "‚úÖ History files committed and pushed to public repo"
          else
            echo "‚ÑπÔ∏è  No changes to commit"
          fi
